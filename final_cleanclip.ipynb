{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88309228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Save path: ./cleanclip_filtered_final_ckpt\n",
      "Checkpoint path provided but not found: finetuneclip_ckpt/finetune_epoch1.pt\n",
      "Filtered train samples: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|          | 1/100 [00:02<03:29,  2.12s/it, loss=2.067570]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 559\u001b[39m\n\u001b[32m    556\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCUDA not available; falling back to cpu.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    557\u001b[39m     args.device = \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 471\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;66;03m# compute loss robustly\u001b[39;00m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     loss, loss_clip_val, loss_ss_val = \u001b[43mcompute_cleanclip_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrozen_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlambda1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlambda2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    473\u001b[39m     tb = traceback.format_exc()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 118\u001b[39m, in \u001b[36mcompute_cleanclip_loss\u001b[39m\u001b[34m(model, frozen_model, images, tokens, device, lambda1, lambda2)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_cleanclip_loss\u001b[39m(model, frozen_model, images, tokens, device, lambda1, lambda2):\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# get features and force FP32 (avoid fp16 overflow)\u001b[39;00m\n\u001b[32m    117\u001b[39m     img_feat = model.encode_image(images).float()\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     txt_feat = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m.float()\n\u001b[32m    119\u001b[39m     \u001b[38;5;66;03m# normalize (with eps)\u001b[39;00m\n\u001b[32m    120\u001b[39m     img_feat = img_feat / (img_feat.norm(dim=-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m) + \u001b[32m1e-10\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\suran\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\clip\\model.py:354\u001b[39m, in \u001b[36mCLIP.encode_text\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    350\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_final(x).type(\u001b[38;5;28mself\u001b[39m.dtype)\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# x.shape = [batch_size, n_ctx, transformer.width]\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# take features from the eot embedding (eot_token is the highest number in each sequence)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m x = x[torch.arange(x.shape[\u001b[32m0\u001b[39m]), text.argmax(dim=-\u001b[32m1\u001b[39m)] @ \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_projection\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\suran\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1951\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1946\u001b[39m         \u001b[38;5;28mself\u001b[39m._backward_pre_hooks = OrderedDict()\n\u001b[32m   1948\u001b[39m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[32m   1949\u001b[39m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[32m   1950\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Union[Tensor, \u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1952\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n\u001b[32m   1953\u001b[39m         _parameters = \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "train_cleanclip_filtered_final.py\n",
    "\n",
    "Robust CleanCLIP fine-tuning on filtered (clean-only) training data.\n",
    "\n",
    "Usage example:\n",
    "  python train_cleanclip_filtered_final.py \\\n",
    "    --data-root catsdogs_dataset \\\n",
    "    --train-csv catsdogs_dataset/train.csv \\\n",
    "    --val-csv catsdogs_dataset/val.csv \\\n",
    "    --checkpoint ./finetuneclip_ckpt/finetune_epoch5.pt \\\n",
    "    --save-path ./cleanclip_filtered_final_ckpt \\\n",
    "    --epochs 3 \\\n",
    "    --batch-size 16 \\\n",
    "    --lr 1e-6 \\\n",
    "    --lambda1 1.0 \\\n",
    "    --lambda2 0.5 \\\n",
    "    --asr-target \"a photo of a banana\" \\\n",
    "    --freeze-logit\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import argparse\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import clip\n",
    "\n",
    "# optional: torchvision for saving preview images\n",
    "try:\n",
    "    import torchvision\n",
    "    _TORCHVISION_AVAILABLE = True\n",
    "except Exception:\n",
    "    _TORCHVISION_AVAILABLE = False\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def norm_caption(s):\n",
    "    return \" \".join(s.lower().strip().split()) if s is not None else \"\"\n",
    "\n",
    "def ensure_dir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def log_bad_sample(save_path, tag, epoch, batch_idx, sample_idx, path, caption, reason, tensor=None):\n",
    "    ensure_dir(save_path)\n",
    "    line = f\"{tag} epoch={epoch} batch={batch_idx} idx={sample_idx} path={path} reason={reason} caption={caption}\\n\"\n",
    "    with open(os.path.join(save_path, \"bad_samples.log\"), \"a\", encoding=\"utf-8\") as fo:\n",
    "        fo.write(line)\n",
    "    try:\n",
    "        if _TORCHVISION_AVAILABLE and tensor is not None:\n",
    "            t = tensor.clone().detach().cpu()\n",
    "            if t.dim() == 3:\n",
    "                t = t.unsqueeze(0)\n",
    "            preview = os.path.join(save_path, f\"bad_{tag}_e{epoch}_b{batch_idx}_i{sample_idx}.png\")\n",
    "            torchvision.utils.save_image(t, preview, normalize=True)\n",
    "    except Exception as e:\n",
    "        with open(os.path.join(save_path, \"bad_samples.log\"), \"a\", encoding=\"utf-8\") as fo:\n",
    "            fo.write(f\"preview_save_failed: {e}\\n\")\n",
    "\n",
    "def safe_preprocess(path, preprocess):\n",
    "    \"\"\"Return (tensor, err). tensor is CxHxW float or None on error.\"\"\"\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        return None, f\"PIL_open_error:{e}\"\n",
    "    try:\n",
    "        t = preprocess(img)  # usually float tensor\n",
    "    except Exception as e:\n",
    "        return None, f\"preprocess_error:{e}\"\n",
    "    if torch.isnan(t).any().item() or torch.isinf(t).any().item():\n",
    "        return None, \"tensor_nan_or_inf\"\n",
    "    vmin = float(t.min().item()); vmax = float(t.max().item())\n",
    "    if vmin < -1e3 or vmax > 1e3:\n",
    "        return None, f\"tensor_out_of_range min={vmin:.2f},max={vmax:.2f}\"\n",
    "    return t, None\n",
    "\n",
    "# -------------------------\n",
    "# Dataset index-only (preprocess in loop for robustness)\n",
    "# -------------------------\n",
    "class CleanIndexDataset(Dataset):\n",
    "    def __init__(self, csv_file, data_root, skip_predicates=None):\n",
    "        self.samples = []\n",
    "        self.data_root = data_root\n",
    "        with open(csv_file, newline='', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            if \"image_path\" not in reader.fieldnames or \"caption\" not in reader.fieldnames:\n",
    "                raise ValueError(\"CSV must contain 'image_path' and 'caption' columns\")\n",
    "            for row in reader:\n",
    "                rel = row[\"image_path\"]\n",
    "                rel_n = rel.replace(\"\\\\\", os.path.sep).replace(\"/\", os.path.sep).lstrip(os.path.sep)\n",
    "                path = os.path.join(data_root, rel_n)\n",
    "                caption = row[\"caption\"]\n",
    "                skip = False\n",
    "                if skip_predicates:\n",
    "                    for pred in skip_predicates:\n",
    "                        if pred(path, caption):\n",
    "                            skip = True\n",
    "                            break\n",
    "                if not skip:\n",
    "                    self.samples.append((path, caption))\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "# -------------------------\n",
    "# CleanCLIP loss: L_CLIP + L_SS\n",
    "# -------------------------\n",
    "def compute_cleanclip_loss(model, frozen_model, images, tokens, device, lambda1, lambda2):\n",
    "    # get features and force FP32 (avoid fp16 overflow)\n",
    "    img_feat = model.encode_image(images).float()\n",
    "    txt_feat = model.encode_text(tokens).float()\n",
    "    # normalize (with eps)\n",
    "    img_feat = img_feat / (img_feat.norm(dim=-1, keepdim=True) + 1e-10)\n",
    "    txt_feat = txt_feat / (txt_feat.norm(dim=-1, keepdim=True) + 1e-10)\n",
    "\n",
    "    # CLIP loss (symmetric)\n",
    "    logit_scale = model.logit_scale.exp().to(dtype=img_feat.dtype)\n",
    "    logits_img = logit_scale * (img_feat @ txt_feat.t())\n",
    "    logits_txt = logits_img.t()\n",
    "    labels = torch.arange(len(images), device=device)\n",
    "    loss_clip = (torch.nn.functional.cross_entropy(logits_img, labels) +\n",
    "                 torch.nn.functional.cross_entropy(logits_txt, labels)) / 2.0\n",
    "\n",
    "    # Stability/self-supervised loss relative to frozen model (no grad)\n",
    "    with torch.no_grad():\n",
    "        frozen_img = frozen_model.encode_image(images).float()\n",
    "        frozen_txt = frozen_model.encode_text(tokens).float()\n",
    "        frozen_img = frozen_img / (frozen_img.norm(dim=-1, keepdim=True) + 1e-10)\n",
    "        frozen_txt = frozen_txt / (frozen_txt.norm(dim=-1, keepdim=True) + 1e-10)\n",
    "    loss_ss = (torch.nn.functional.mse_loss(img_feat, frozen_img) +\n",
    "               torch.nn.functional.mse_loss(txt_feat, frozen_txt)) / 2.0\n",
    "\n",
    "    total = lambda1 * loss_clip + lambda2 * loss_ss\n",
    "    return total, float(loss_clip.detach().cpu().item()), float(loss_ss.detach().cpu().item())\n",
    "\n",
    "# -------------------------\n",
    "# Evaluation and ASR helpers\n",
    "# -------------------------\n",
    "def evaluate_and_save_preds(model, val_index_ds, preprocess, device, save_path, epoch, eval_batch_size=64):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    pred_rows = []\n",
    "    val_loader = DataLoader(list(range(len(val_index_ds))), batch_size=eval_batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_indices in val_loader:\n",
    "            imgs = []\n",
    "            caps = []\n",
    "            paths = []\n",
    "            for ds_idx in batch_indices:\n",
    "                p, c = val_index_ds[ds_idx]\n",
    "                t, err = safe_preprocess(p, preprocess)\n",
    "                if err:\n",
    "                    log_bad_sample(save_path, \"val\", epoch, 0, ds_idx, p, c, \"val_preprocess_err:\"+err, tensor=t)\n",
    "                    continue\n",
    "                imgs.append(t)\n",
    "                caps.append(c)\n",
    "                paths.append(p)\n",
    "            if len(imgs) == 0:\n",
    "                continue\n",
    "\n",
    "            images = torch.stack(imgs, dim=0).to(device)\n",
    "            tokens = clip.tokenize(caps, truncate=True).to(device)\n",
    "\n",
    "            # clamp logit_scale\n",
    "            with torch.no_grad():\n",
    "                model.logit_scale.data = torch.clamp(model.logit_scale.data, min=-5.0, max=4.0)\n",
    "\n",
    "            try:\n",
    "                image_features = model.encode_image(images).float()\n",
    "                text_features = model.encode_text(tokens).float()\n",
    "            except Exception as e:\n",
    "                with open(os.path.join(save_path, \"bad_samples.log\"), \"a\", encoding=\"utf-8\") as fo:\n",
    "                    fo.write(f\"eval epoch={epoch} encode_exception: {e}\\n\")\n",
    "                continue\n",
    "\n",
    "            image_features = image_features / (image_features.norm(dim=-1, keepdim=True) + 1e-10)\n",
    "            text_features = text_features / (text_features.norm(dim=-1, keepdim=True) + 1e-10)\n",
    "            logit_scale = model.logit_scale.exp().to(dtype=image_features.dtype)\n",
    "            sims = (logit_scale * (image_features @ text_features.t()))\n",
    "            preds = sims.argmax(dim=1).tolist()\n",
    "\n",
    "            for i, pred_idx in enumerate(preds):\n",
    "                pred_caption = caps[pred_idx]\n",
    "                gold = caps[i]\n",
    "                correct = int(norm_caption(pred_caption) == norm_caption(gold))\n",
    "                pred_rows.append({\n",
    "                    \"image_path\": paths[i],\n",
    "                    \"gold_caption\": gold,\n",
    "                    \"pred_caption\": pred_caption,\n",
    "                    \"correct\": correct\n",
    "                })\n",
    "                total_correct += correct\n",
    "                total_samples += 1\n",
    "\n",
    "    acc = total_correct / max(1, total_samples)\n",
    "    ensure_dir(save_path)\n",
    "    csv_path = os.path.join(save_path, f\"cleanclip_preds_epoch{epoch}.csv\")\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"image_path\", \"gold_caption\", \"pred_caption\", \"correct\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(pred_rows)\n",
    "    return acc, csv_path, pred_rows\n",
    "\n",
    "def compute_asr_from_preds(pred_rows, val_csv, data_root, save_path, target):\n",
    "    pred_map = {}\n",
    "    for r in pred_rows:\n",
    "        p = r[\"image_path\"].replace(\"\\\\\", os.path.sep).replace(\"/\", os.path.sep)\n",
    "        pred_map[p] = r[\"pred_caption\"]\n",
    "\n",
    "    val_meta = []\n",
    "    with open(val_csv, newline='', encoding='utf-8') as vf:\n",
    "        vr = csv.DictReader(vf)\n",
    "        for r in vr:\n",
    "            rel = r.get(\"image_path\", \"\")\n",
    "            rel_n = rel.replace(\"\\\\\", os.path.sep).replace(\"/\", os.path.sep).lstrip(os.path.sep)\n",
    "            fullp = os.path.join(data_root, rel_n)\n",
    "            is_poison = False\n",
    "            for key in [\"poisoned\",\"poisoned_from\",\"is_poison\",\"poison\",\"poison_flag\"]:\n",
    "                if key in r and str(r.get(key)).strip().lower() not in [\"\", \"nan\", \"none\", \"false\", \"0\"]:\n",
    "                    is_poison = True\n",
    "                    break\n",
    "            if not is_poison and \"poison\" in rel_n.lower():\n",
    "                is_poison = True\n",
    "            val_meta.append({\"path\": fullp, \"caption\": r.get(\"caption\",\"\"), \"poisoned\": is_poison})\n",
    "\n",
    "    target_norm = norm_caption(target)\n",
    "    poisoned_entries = []\n",
    "    for vm in val_meta:\n",
    "        if not vm[\"poisoned\"]:\n",
    "            continue\n",
    "        pred_caption = pred_map.get(vm[\"path\"], \"\")\n",
    "        is_target = int(norm_caption(pred_caption) == target_norm)\n",
    "        poisoned_entries.append({\n",
    "            \"image_path\": vm[\"path\"],\n",
    "            \"gold_caption\": vm[\"caption\"],\n",
    "            \"pred_caption\": pred_caption,\n",
    "            \"is_target\": is_target\n",
    "        })\n",
    "\n",
    "    total_poison = len(poisoned_entries)\n",
    "    total_success = sum([e[\"is_target\"] for e in poisoned_entries])\n",
    "    asr = (total_success / total_poison) if total_poison>0 else float(\"nan\")\n",
    "    out_csv = os.path.join(save_path, f\"cleanclip_asr_details.csv\")\n",
    "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as af:\n",
    "        writer = csv.DictWriter(af, fieldnames=[\"image_path\",\"gold_caption\",\"pred_caption\",\"is_target\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(poisoned_entries)\n",
    "    return asr, total_success, total_poison, out_csv\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "import copy\n",
    "import time\n",
    "import csv\n",
    "\n",
    "def run_ablation(base_args):\n",
    "    \"\"\"\n",
    "    Run ablation study over loss weights:\n",
    "      (1) Default: lambda1=1.0, lambda2=base_args.lambda2\n",
    "      (2) Only stability loss: lambda1=0.0, lambda2=base_args.lambda2\n",
    "      (3) Only CLIP loss: lambda1=1.0, lambda2=0.0\n",
    "\n",
    "    Saves results to ablation_results.csv under base_args.save_path.\n",
    "    \"\"\"\n",
    "\n",
    "    ablation_settings = [\n",
    "        {\"name\": \"baseline\", \"lambda1\": base_args.lambda1, \"lambda2\": base_args.lambda2},\n",
    "        {\"name\": \"stability_only\", \"lambda1\": 0.0, \"lambda2\": base_args.lambda2},\n",
    "        {\"name\": \"clip_only\", \"lambda1\": base_args.lambda1, \"lambda2\": 0.0},\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    csv_path = os.path.join(base_args.save_path, \"ablation_results.csv\")\n",
    "    ensure_dir(base_args.save_path)\n",
    "\n",
    "    for config in ablation_settings:\n",
    "        print(\"\\n==============================\")\n",
    "        print(f\" Running Ablation: {config['name']} \")\n",
    "        print(f\" lambda1={config['lambda1']}  lambda2={config['lambda2']}\")\n",
    "        print(\"==============================\\n\")\n",
    "\n",
    "        args = copy.deepcopy(base_args)\n",
    "        args.lambda1 = config[\"lambda1\"]\n",
    "        args.lambda2 = config[\"lambda2\"]\n",
    "\n",
    "        # give each run its own subfolder\n",
    "        args.save_path = os.path.join(base_args.save_path, f\"ablation_{config['name']}\")\n",
    "        ensure_dir(args.save_path)\n",
    "\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            main(args)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Ablation '{config['name']}' failed: {e}\")\n",
    "            continue\n",
    "        end_time = time.time()\n",
    "        elapsed = round(end_time - start_time, 2)\n",
    "\n",
    "        # read last preds csv and asr file if exist\n",
    "        last_pred_csv = None\n",
    "        last_asr_csv = None\n",
    "        acc = asr = succ = tot = float(\"nan\")\n",
    "\n",
    "        # try to find last epoch file\n",
    "        all_files = os.listdir(args.save_path)\n",
    "        epoch_nums = []\n",
    "        for fn in all_files:\n",
    "            if fn.startswith(\"cleanclip_preds_epoch\") and fn.endswith(\".csv\"):\n",
    "                try:\n",
    "                    ep = int(fn.replace(\"cleanclip_preds_epoch\", \"\").replace(\".csv\", \"\"))\n",
    "                    epoch_nums.append(ep)\n",
    "                except:\n",
    "                    pass\n",
    "        if epoch_nums:\n",
    "            last_ep = max(epoch_nums)\n",
    "            last_pred_csv = os.path.join(args.save_path, f\"cleanclip_preds_epoch{last_ep}.csv\")\n",
    "            last_asr_csv = os.path.join(args.save_path, f\"cleanclip_asr_details.csv\")\n",
    "            # parse val accuracy\n",
    "            with open(last_pred_csv, newline='', encoding='utf-8') as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                preds = list(reader)\n",
    "            if preds:\n",
    "                acc = sum(int(r[\"correct\"]) for r in preds) / len(preds)\n",
    "            # parse ASR file\n",
    "            if os.path.exists(last_asr_csv):\n",
    "                with open(last_asr_csv, newline='', encoding='utf-8') as f:\n",
    "                    reader = csv.DictReader(f)\n",
    "                    asr_rows = list(reader)\n",
    "                    if asr_rows:\n",
    "                        succ = sum(int(r[\"is_target\"]) for r in asr_rows)\n",
    "                        tot = len(asr_rows)\n",
    "                        asr = succ / tot if tot > 0 else float(\"nan\")\n",
    "\n",
    "        results.append({\n",
    "            \"name\": config[\"name\"],\n",
    "            \"lambda1\": args.lambda1,\n",
    "            \"lambda2\": args.lambda2,\n",
    "            \"val_accuracy\": acc,\n",
    "            \"ASR\": asr,\n",
    "            \"ASR_success\": succ,\n",
    "            \"ASR_total\": tot,\n",
    "            \"runtime_sec\": elapsed,\n",
    "            \"pred_csv\": last_pred_csv,\n",
    "            \"asr_csv\": last_asr_csv,\n",
    "        })\n",
    "\n",
    "        print(f\"✅ Done: {config['name']} | Acc={acc*100:.2f}% | ASR={asr*100:.2f}% | Time={elapsed}s\")\n",
    "\n",
    "    # save all results\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(results[0].keys()))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "    print(\"\\n=== Ablation summary ===\")\n",
    "    for r in results:\n",
    "        print(f\"{r['name']}: acc={r['val_accuracy']*100:.2f}%  asr={r['ASR']*100:.2f}%  (λ1={r['lambda1']} λ2={r['lambda2']})\")\n",
    "    print(f\"\\nSaved all results to {csv_path}\\n\")\n",
    "\n",
    "def main(args):\n",
    "    # make sure save_path exists early (avoid FileNotFoundError on logging)\n",
    "    ensure_dir(args.save_path)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() and args.device==\"cuda\" else \"cpu\"\n",
    "    print(\"Device:\", device)\n",
    "    print(\"Save path:\", args.save_path)\n",
    "\n",
    "    # frozen reference model: pretrained CLIP\n",
    "    frozen_model, preprocess = clip.load(args.clip_backbone, device=device, jit=False)\n",
    "    frozen_model.eval()\n",
    "    frozen_model.float()\n",
    "\n",
    "    # trainable model: load pretrained and checkpoint into it\n",
    "    model, _ = clip.load(args.clip_backbone, device=device, jit=False)\n",
    "    # force FP32 params\n",
    "    model.float()\n",
    "\n",
    "    # reset/clamp logit_scale to safe starting value\n",
    "    with torch.no_grad():\n",
    "        model.logit_scale.data.fill_(0.0)   # exp(0)=1.0\n",
    "        model.logit_scale.data.clamp_(-2.0, 2.0)\n",
    "\n",
    "    # optionally load checkpoint (partial allowed)\n",
    "    if args.checkpoint:\n",
    "        if os.path.exists(args.checkpoint):\n",
    "            print(\"Loading checkpoint into model:\", args.checkpoint)\n",
    "            state = torch.load(args.checkpoint, map_location=device)\n",
    "            model.load_state_dict(state, strict=False)\n",
    "        else:\n",
    "            print(\"Checkpoint path provided but not found:\", args.checkpoint)\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # decide optimizer param groups: optionally freeze logit_scale or give tiny lr\n",
    "    if args.freeze_logit:\n",
    "        for n,p in model.named_parameters():\n",
    "            if \"logit_scale\" in n:\n",
    "                p.requires_grad = False\n",
    "        opt_params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.AdamW(opt_params, lr=args.lr, weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        # separate group for logit_scale with tiny LR\n",
    "        logit_params = [p for n,p in model.named_parameters() if \"logit_scale\" in n and p.requires_grad]\n",
    "        other_params = [p for n,p in model.named_parameters() if \"logit_scale\" not in n and p.requires_grad]\n",
    "        groups = [{\"params\": other_params, \"lr\": args.lr}, {\"params\": logit_params, \"lr\": args.logit_lr}]\n",
    "        optimizer = torch.optim.AdamW(groups, weight_decay=args.weight_decay)\n",
    "\n",
    "    # build filtered train index dataset (skip \"poison\", \"banana\", \"sketch\")\n",
    "    def skip_pred(path, caption):\n",
    "        lo_caption = (caption or \"\").lower()\n",
    "        lo_path = (path or \"\").lower()\n",
    "        if \"poison\" in lo_path: return True\n",
    "        if \"banana\" in lo_caption: return True\n",
    "        if \"sketch\" in lo_caption: return True\n",
    "        return False\n",
    "\n",
    "    train_index_ds = CleanIndexDataset(args.train_csv, args.data_root, skip_predicates=[skip_pred])\n",
    "    val_index_ds = CleanIndexDataset(args.val_csv, args.data_root, skip_predicates=[])  # keep val full\n",
    "\n",
    "    print(\"Filtered train samples:\", len(train_index_ds))   \n",
    "\n",
    "    # loaders built as list-of-indices and preprocessed inside loop for robustness\n",
    "    train_loader = DataLoader(list(range(len(train_index_ds))), batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_clip = 0.0\n",
    "        total_ss = 0.0\n",
    "        steps = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{args.epochs}\")\n",
    "        for batch_idx, batch_indices in enumerate(pbar):\n",
    "            imgs = []\n",
    "            caps = []\n",
    "            paths = []\n",
    "            for local_idx, ds_idx in enumerate(batch_indices):\n",
    "                path, caption = train_index_ds[ds_idx]\n",
    "                t, err = safe_preprocess(path, preprocess)\n",
    "                if err:\n",
    "                    log_bad_sample(args.save_path, \"train\", epoch, batch_idx, local_idx, path, caption, \"preprocess_err:\"+err, tensor=t)\n",
    "                    continue\n",
    "                imgs.append(t)\n",
    "                caps.append(caption)\n",
    "                paths.append(path)\n",
    "            if len(imgs) == 0:\n",
    "                with open(os.path.join(args.save_path, \"bad_samples.log\"), \"a\", encoding=\"utf-8\") as fo:\n",
    "                    fo.write(f\"epoch={epoch} batch={batch_idx} all_bad_samples_skipped\\n\")\n",
    "                continue\n",
    "\n",
    "            images = torch.stack(imgs, dim=0).to(device)\n",
    "            tokens = clip.tokenize(caps, truncate=True).to(device)\n",
    "\n",
    "            # clamp logit_scale per batch\n",
    "            with torch.no_grad():\n",
    "                model.logit_scale.data = torch.clamp(model.logit_scale.data, min=-5.0, max=4.0)\n",
    "\n",
    "            # compute loss robustly\n",
    "            try:\n",
    "                loss, loss_clip_val, loss_ss_val = compute_cleanclip_loss(model, frozen_model, images, tokens, device, args.lambda1, args.lambda2)\n",
    "            except Exception as e:\n",
    "                tb = traceback.format_exc()\n",
    "                with open(os.path.join(args.save_path, \"bad_samples.log\"), \"a\", encoding=\"utf-8\") as fo:\n",
    "                    fo.write(f\"epoch={epoch} batch={batch_idx} forward_exception: {e}\\n{tb}\\n\")\n",
    "                # isolate per-sample to find offending ones\n",
    "                for i_sample, (pth, cap) in enumerate(zip(paths, caps)):\n",
    "                    t_single, err2 = safe_preprocess(pth, preprocess)\n",
    "                    if err2:\n",
    "                        log_bad_sample(args.save_path, \"train_isolate\", epoch, batch_idx, i_sample, pth, cap, \"preprocess_err:\"+err2, tensor=t_single)\n",
    "                        continue\n",
    "                    try:\n",
    "                        t_b = t_single.unsqueeze(0).to(device)\n",
    "                        tok_b = clip.tokenize([cap]).to(device)\n",
    "                        _loss, _, _ = compute_cleanclip_loss(model, frozen_model, t_b, tok_b, device, args.lambda1, args.lambda2)\n",
    "                    except Exception as ee:\n",
    "                        log_bad_sample(args.save_path, \"train_isolate\", epoch, batch_idx, i_sample, pth, cap, f\"forward_exception:{ee}\", tensor=t_single)\n",
    "                continue\n",
    "\n",
    "            # catch NaN/Inf\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                with open(os.path.join(args.save_path, \"bad_samples.log\"), \"a\", encoding=\"utf-8\") as fo:\n",
    "                    fo.write(f\"epoch={epoch} batch={batch_idx} loss_nan_or_inf - skipped\\n\")\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if args.grad_clip and args.grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=args.grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += float(loss.detach().cpu().item())\n",
    "            total_clip += float(loss_clip_val)\n",
    "            total_ss += float(loss_ss_val)\n",
    "            steps += 1\n",
    "            pbar.set_postfix(loss=f\"{(total_loss/steps):.6f}\")\n",
    "\n",
    "        if steps > 0:\n",
    "            print(f\"Epoch {epoch} avg loss {total_loss/steps:.6f} (clip={total_clip/steps:.6f}, ss={total_ss/steps:.6f})\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch} had 0 successful steps.\")\n",
    "\n",
    "        # checkpoint\n",
    "        ensure_dir(args.save_path)\n",
    "        ckpt = os.path.join(args.save_path, f\"cleanclip_filtered_epoch{epoch}.pt\")\n",
    "        torch.save(model.state_dict(), ckpt)\n",
    "        print(\"Saved checkpoint:\", ckpt)\n",
    "\n",
    "        # evaluate + ASR\n",
    "        acc, preds_csv, pred_rows = evaluate_and_save_preds(model, val_index_ds, preprocess, device, args.save_path, epoch, eval_batch_size=args.eval_batch_size)\n",
    "        print(f\"Validation accuracy (per-sample) after epoch {epoch}: {acc*100:.2f}%\")\n",
    "        asr, succ, tot, asr_csv = compute_asr_from_preds(pred_rows, args.val_csv, args.data_root, args.save_path, args.asr_target)\n",
    "        print(f\"ASR (target='{args.asr_target}'): {asr*100:.2f}% ({succ}/{tot})  details -> {asr_csv}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "# -------------------------\n",
    "# CLI\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    from argparse import Namespace\n",
    "    args = Namespace(\n",
    "        data_root=\"catsdogs_dataset\",\n",
    "        train_csv=\"catsdogs_dataset/train2.csv\",\n",
    "        val_csv=\"catsdogs_dataset/val.csv\",\n",
    "        checkpoint=\"finetuneclip_ckpt/finetune_epoch1.pt\",\n",
    "        save_path=\"./cleanclip_filtered_final_ckpt\",\n",
    "        clip_backbone=\"ViT-B/32\",\n",
    "        batch_size=8,\n",
    "        eval_batch_size=8,\n",
    "        epochs=1,\n",
    "        lr=5e-6,\n",
    "        logit_lr=1e-8,\n",
    "        weight_decay=1e-4,\n",
    "        lambda1=1.0,\n",
    "        lambda2=8,\n",
    "        grad_clip=1.0,\n",
    "        freeze_logit=False,\n",
    "        device=\"cuda\",\n",
    "        asr_target=\"This is a sketch of banana\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # fallback for device\n",
    "    if args.device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(\"CUDA not available; falling back to cpu.\")\n",
    "        args.device = \"cpu\"\n",
    "\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12bc729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
